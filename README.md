# README.md

MNIST Data

For this lab we had to create a deep learning model which utilized the MNIST dataset which is already preloaded in the Keras library. The data consists of 70,000 handwriting samples. This model was the first model we are using that contains a larger dataset. We begun the lab by splitting the data into a testing dataset and a training dataset. We used 10,000 samples in the test data while including the remaining 60,000 in the training data. We then learned how to import models and layers from keras an essential step when creating a deep learning model. This lab put a focus on the activation function and its effect. We usef the 'relu' activation function for this model and then finished up by using the 'softmax' function which is also classified as an activation function which normalizes the output of the model to a probability distribution over predicted output classes. This lab also wanted us to scale the data between values 0 and 1 which improves the effiency of the model. We then began to train the model using  'rmsprop' for our optimizer and 'categorical_crossentropy' for our loss function. We were then able to run our model and the total error and accuracy. Which returned a 97.8% accuracy. The second part of this lab was to have Chat-Gpt create the same model itself and compare its performance. The Chat-Gpt model was able to achieve a 99.09% accuracy score. We believe this is because Chat-Gpt chose a better optimizer function which fit the data better. 
