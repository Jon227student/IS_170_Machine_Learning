# README.md

MNIST Data

In this lab, we were tasked with creating a deep learning model utilizing the MNIST dataset, which is preloaded in the Keras library. The dataset comprises 70,000 handwriting samples. This model was the first one we used that involves a larger dataset. We began the lab by dividing the data into testing and training datasets, allocating 10,000 samples to the test data and the remaining 60,000 to the training data. We then learned how to import models and layers from Keras, an essential step in creating a deep learning model. This lab emphasized the impact of the activation function. We employed the 'relu' activation function for this model, and concluded by using the 'softmax' function, which is also classified as an activation function. The 'softmax' function normalizes the model's output to a probability distribution over predicted output classes. We were also instructed to scale the data between values 0 and 1 to enhance the model's efficiency. Subsequently, we began training the model, using 'rmsprop' as our optimizer and 'categorical_crossentropy' as our loss function. Upon running our model, we were able to ascertain the total error and accuracy, which yielded a 97.8% accuracy rate. The second part of the lab entailed having Chat-GPT create the same model independently and compare its performance. The Chat-GPT model achieved a 99.09% accuracy score. We surmise that this higher accuracy was due to Chat-GPT selecting a more fitting optimizer function for the data.
